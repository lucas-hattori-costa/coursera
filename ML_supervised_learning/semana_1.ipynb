{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semana 1 - Classification using Decision Trees and kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees\n",
    "\n",
    "    Decision trees é um modelo linear que vai splitando o espaço das features em pequenos espaços limitados por planos, ou seja, escolhas binárias\n",
    "    Ponto negativo -> tende a ser muito instável (porque?) então usa-ser random forests\n",
    "\n",
    "> Random Forest\n",
    "    \n",
    "    Conjunto de decision trees que são treinadas com subsets do train_data. Assim, são geradas várias arvóres e, para cada predição da floresta, temos várias predições, uma de cada árvore; a predição da floresta é a predição mais recorrente das árvores\n",
    "    \n",
    "> Overfitting\n",
    "\n",
    "    Para evitar overfitting em decisions trees, podemos olhar 3 parâmetros: max_leaf_nodes (número máximo de pontos finais da árvore), min_samples_leaf (número mínimo de exemplos em cada leaf), max_depht (máxima distancia entre o input e a leaf). No geral, esses parâmetros tendem a reduzir o tamanho das árvores e reduzir overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### $k$-NN\n",
    "\n",
    "    Método k-NN pega os k nearest neighbors de um ponto e classifica ele na classe mais frequente dentre esses k vizinhos\n",
    "\n",
    "> Distance measures\n",
    "    \n",
    "    A mais tradicional é a euclidiana. Outros tipos podem ser a manhatan (L1 - norm) e a Hamming distance (boa para categorical features, devolve 1 se for diferente e 0 se for igual). Mahalanobis é uma measure probabilística.\n",
    "\n",
    "> Tipos de features\n",
    "    \n",
    "    Aqui, faz diferença se as escalas das features forem muito diferentes, pq a distance measure pode dar mais peso pra algumas em detrimento de outras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semana 2 - Regressão linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "> Loss functions\n",
    "\n",
    "    - Least Square Error: sum (y - y_hat)^2 \n",
    "    Permitem achar as melhores hipóteses dentro do espaço de hipóteses\n",
    "\n",
    "> Gradient Descent\n",
    "    \n",
    "    Calcula o gradiente da função Loss e ajusta os parâmetros da regressão com base nele e em um alpha que é um hiperparâmetro de learning rate\n",
    "    \n",
    "> Polynomial Feature Expansion\n",
    "\n",
    "    Tipo de feature engineering que aumenta o número de features ao multiplicar as features originais. Então o modelo que vc está prevendo vai descrever um espaço de grau maior do que 1, mas os parâmetros que vc está calculando usando gradient descent são de ordem 1.\n",
    "    \n",
    "> Bias and variance tradeoff\n",
    "\n",
    "    Modo de reconhecer: High Bias vai performar mal inclusive com training data. High Variance indica overfitting e alta taxa de acerto com a training data. Para verificar isso, treinar de novo com outros subsets do training data.\n",
    "\n",
    "> Regularizers\n",
    "    \n",
    "    Um modo de controlar o bias and variance tradeoff.Regularizers basicamente criam penalties para modelos muito complexos de high variance. Ridge regression é uma linear regression usando um regularizer L2. Esse regularizer também usa um parâmetro lambda que é smoothing, ou seja, controla o quão complexo vc quer que seu modelo seja. Um regularizer L2 pega a norma L2 dos weights, penalizando assim, os modelos de weights em coeficientes muito altos. Lasso regression utiliza um regularizer L1.\n",
    "    Basicamente, a ideia é que coeficientes menores descrevem sympler hypothesis e que vc pode reduzir a dimensionalidade da sua solução se vc multiplicar os coeficientes de maior ordem por um valor bem pequeno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semana 3 - Regression for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelos com transfer function\n",
    "\n",
    "#### Logistic regression\n",
    "\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM method)\n",
    "\n",
    "> Hinge Loss\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
